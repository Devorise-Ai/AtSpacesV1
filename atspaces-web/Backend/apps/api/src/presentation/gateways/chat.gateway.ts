import {
    WebSocketGateway,
    SubscribeMessage,
    MessageBody,
    ConnectedSocket,
    OnGatewayConnection,
    OnGatewayDisconnect,
    WebSocketServer,
} from '@nestjs/websockets';
import { Server, Socket } from 'socket.io';
import { AgentService } from '../../infrastructure/services/agent.service';
import { Logger } from '@nestjs/common';

@WebSocketGateway({
    cors: {
        origin: '*', // Allow connections from frontend
    },
})
export class ChatGateway implements OnGatewayConnection, OnGatewayDisconnect {
    @WebSocketServer()
    server: Server;

    private readonly logger = new Logger(ChatGateway.name);

    constructor(private readonly agentService: AgentService) { }

    handleConnection(client: Socket, ...args: any[]) {
        this.logger.log(`Client connected: ${client.id}`);
    }

    handleDisconnect(client: Socket) {
        this.logger.log(`Client disconnected: ${client.id}`);
    }

    @SubscribeMessage('sendMessage')
    async handleMessage(
        @MessageBody() data: { message: string, sessionId?: string, token?: string },
        @ConnectedSocket() client: Socket,
    ): Promise<void> {
        const sessionId = data.sessionId || client.id;
        this.logger.log(`Running agent for session: ${sessionId}, Payload received: ${JSON.stringify({ ...data, token: data.token ? 'PRESENT' : 'MISSING' })}`);

        try {
            const stream = this.agentService.streamChat(sessionId, data.message, data.token);

            // Stream each chunk back to the client as it's generated by LangGraph / OpenAI
            for await (const chunk of stream) {
                client.emit('messageChunk', { chunk });
            }

            // Signal that the token generation is complete
            client.emit('messageComplete', { sessionId });
        } catch (error) {
            this.logger.error('Error generating AI response', error);
            client.emit('messageError', { error: 'Failed to process AI response.' });
        }
    }
}
